{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6867cb-dacb-4115-a013-982c5de3edb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 1 : Problem Definition, Kaggle Competition selected and why\n",
    "\n",
    "I chose the following kaggle competition : https://www.kaggle.com/competitions/playground-series-s3e6/data?select=test.csv .\n",
    "The dataset is based on the Paris Housing Price Prediction dataset (https://www.kaggle.com/datasets/mssmartypants/paris-housing-price-prediction), which is an imaginary dataset of house prices in an urban environment. This dataset was generated from a deep learning model trained on that dataset. \n",
    "\n",
    "The features in this dataset are the following : id, squareMeters, numberOfRooms, hasYard, hasPool, floors, cityCode, cityPartRange, numPrevOwners, made, isNewBuilt, hasStormProtector, basement, attic, garage, hasStorageRoom and hasGuestRoom. they are used to preict the target variable price.\n",
    "\n",
    "I chose a regression task as the majority of datasets I have worked on previously were for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5911c2d5-2895-498d-afaf-f3ec0b8f8a20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 2 : Data Exploration, Feature Engineering and Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c16ab8fe-89f9-4e98-ad6d-8d147fab4b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we load the dataset, that has been uploded into databricks(through the create table) and define the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb655c94-61b0-4183-b789-b864a46daa90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#definition of the schema, we pick the right data types for each column. For the boolean columns, we choose to load them as integers directly to make the creating of models easier (no need to do One Hot Encoding)\n",
    "schema = 'id INTEGER, squareMeters INTEGER, numberOfRooms INTEGER, hasYard INTEGER, hasPool INTEGER, floors INTEGER, cityCode INTEGER, cityPartRange INTEGER, numPrevOwners INTEGER, made INTEGER, isNewBuilt INTEGER, hasStormProtector INTEGER, basement INTEGER, attic INTEGER, garage INTEGER, hasStorageRoom INTEGER, hasGuestRoom INTEGER, price DOUBLE'\n",
    "\n",
    "df = spark.read.csv(\"/FileStore/tables/train.csv\", schema=schema, header = True) #we read the file into a spark dataframe\n",
    "display(df) #and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5309740a-3930-4d30-a7e6-8f241a58a8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The id column is not meaningful as it just represents the index of the row, so we can drop that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550f6e2c-27d4-4344-bea2-9594b5424da0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.drop(\"id\") # we drop the id column from the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c0ae8fc-9dcb-497d-9ce9-9cf0faa96fe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are no null values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f171f455-cec6-4160-aea5-b20270025f36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data exploration\n",
    "\n",
    "Now we can move onto the data exploration phase to get a better picture of our dataset and the more import features.  \n",
    "\n",
    "We can get the descriptive statistics for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72905ef5-e758-4cd9-b84b-46fe93ee2517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afadc027-dd2b-441a-b15b-9eaff3aff783",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can look at the distribution of each feature through visuals. We will plot the histogram and boxplot associated to each distribution. We will use Databrick's graph functionality. First we need to create an SQL table which we name dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aba77c9-0994-4ff5-8957-f34d7fe1c402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3fe12e-d203-41a2-be5e-b0526dbe130e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We use a simple SQL querry to group the values of squareMeters and count the number of instances associated to each value. We can can see the distribution is very left skewed and there is only one outlier which is very far from the rest of the other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c6f56e-e9da-486a-8348-7d44606a98c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT squareMeters, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY squareMeters ORDER BY squareMeters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6401321-36e9-4d22-a7dc-a08d84c43bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can isolate the outliers, by taking the values higher than Q3 + 1.5 * IQR, the uperbound used in statistics to find outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41681e5-12ec-4e91-a39d-6da8b230e3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT squareMeters, count(*) AS count\n",
    "FROM dataset WHERE squareMeters > 146181 /* 146181 corresponds to Q3 + 1.5 *IQR */\n",
    "GROUP BY squareMeters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a582ce6-3fa9-4a3a-854b-1beff8fbef4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This value of squareMeters is extremely high and seems unlikely. We can save the outliers in a seperate dataframe to remove them from our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e191ed2-1e41-4420-83b2-13b30b9008cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "outliers = df.filter(col(\"squareMeters\") > 146181)\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4d57bc-059e-4462-9376-b291f8c205e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's have a look at numberOfRooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5357cc6f-58a4-4c14-be0e-477319d0a072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT numberOfRooms, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY numberOfRooms\n",
    "ORDER BY numberOfRooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff3eee9-29fe-4805-8208-f0fc6524aaaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here the distribution does not seem approximately normal and there are no outliers.  \n",
    "\n",
    "Now we can move onto the hasYard and hasPool columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09be33e6-6e79-4ff7-8996-70512c8433d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT hasYard, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY hasYard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295bae63-bd7c-4a8e-a165-4f376640bc83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT hasPool, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY hasPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6294dd1b-412b-48b7-9cd2-dbac570b291b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Both columns seem to be more or less balanced, with slightly more houses with no yard or no pool.  \n",
    "\n",
    "Now we can have a look at the floors column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68102f73-b7c5-4d39-9498-829cb9509ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT floors, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY floors\n",
    "ORDER BY floors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e43e2c-75a1-4c9d-95dc-77948c15ab75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The histogram is left skewed and we can notice a single outlier from looking at the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a975dda-c0d8-472b-a559-3a4a88de077f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT floors, count(*) AS count\n",
    "FROM dataset WHERE floors > 100/* 100 corresponds to Q3 + 1.5 * IQR */\n",
    "GROUP BY floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01dfccf0-d7e7-4059-8969-963dabb64927",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"floors\") > 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2005195a-37bd-4aeb-87c0-9f105718a48d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This value does not seem reasonable at all, so we can also remove this outlier, so we add it to the outliers dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5a4107-58ad-4ffe-a829-f6f106816e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"floors\") > 100))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec1ba758-c676-4721-901f-004771b7e3da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's look at the cityCode column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9463620c-e4dd-4e16-b070-cb14e6e02180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT cityCode, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY cityCode\n",
    "ORDER BY cityCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89b622bb-4218-4103-88ec-d5ae912d4f73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the distribution is not a normal distribution and there are a few outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b31e358-78c1-410c-967d-6e81614d7846",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"cityCode\") > 146275))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ba039e-90ab-473e-82d1-06996e950572",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These are very high values, so we will also remove these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bd38f8-a165-4d31-b95f-463993474800",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"cityCode\") > 146275))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22942e8-bd67-4017-9474-0af9f0a06e62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the cityPartRange column and the numPrevOwners, the distributions also do not seem approximately normal and there are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56731c3-ccd6-44c3-8824-eca84c2f1101",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT cityPartRange, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY cityPartRange\n",
    "ORDER BY cityPartRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d03c242-b435-476b-9f52-b9f0764cecdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT numPrevOwners, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY numPrevOwners\n",
    "ORDER BY numPrevOwners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de67404-9306-4a01-b5f9-cf90098cd091",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the made column the distribution is not normally distributed and there are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b4730a-a40f-4768-9989-4e5ff6a04ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT made, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY made\n",
    "ORDER BY made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbdaa94-d4f5-43f5-baf3-e0bfe8168738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"made\") > 2021))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ee4a54-cb7c-40e1-98f0-c4e80031d319",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is not possible for the houses to be made in 10000(this is probaby a data error or anomaly), so we will remove these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a55028-2e68-4e56-8400-cd24db85f286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"made\") > 2021))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60f68e2-7874-4eef-885b-71ad5ee7890c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The isNewBuilt and hasStormProtector columns are evenly balanced with slightly more houses without these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b5e4ab-0a93-4a25-bccc-123b835f9d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT isNewBuilt, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY isNewBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f757c16c-18be-4549-a093-d70287db1be2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT hasStormProtector, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY hasStormProtector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e8bd3f-a7ec-420f-940b-c0138499de01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's look at the basement column's distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8128b137-f2e5-484a-9d04-85289110545b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT basement, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY basement\n",
    "ORDER BY basement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a9003cb-b398-4127-a22b-78a3c69ac29a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can notice that the distribution does not seem normally distributed and there are a few outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e27c1b-505b-4809-9c61-f80248f73d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"basement\") > 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67f7104-2e1e-4041-9a67-0cd9237ec6bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These values seem abnormally high, so we will add them to the outliers to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7a4e03-34fd-4db0-8773-3cbf075c42b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"basement\") > 10000))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ff7a53-05a6-4b66-bb12-474d2bb0f6e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now moving on to the attic column, we can notice it seems mostly normally distributed and there are a few outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d06064-fc5d-4191-8d98-c4bc30239d1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT attic, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY attic\n",
    "ORDER BY attic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03037985-a867-45c9-af2e-f19a9b987720",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"attic\") > 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ff74ec1-71ee-4559-8624-ec8b69d85ee8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We choose to remove these outliers as they don't seem like reasonable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f7ea3e-636f-4faa-990e-b37eb4d004d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"attic\") > 10000))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e3ca1e-d3da-424b-a48a-b8ab07134473",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the garage column, we have an approximately normally distributed feature and there are a few outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f783e1-22b6-48ac-8e3a-39a9ed5a0ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT garage, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY garage\n",
    "ORDER BY garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75929b63-63e1-4b0c-a5a7-baf712ba4d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.filter(col(\"garage\") > 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d028903-28f9-457f-86e8-06b810c95906",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can remove these outliers as they don't seem consistent with the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b20ac1-0072-48a4-93ad-f010e4687f94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "outliers = outliers.union(df.filter(col(\"garage\") > 1000))\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0344f4-d609-4458-9934-1d28e2c18d37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The hasStorageRoom column seems almost evenly balanced with slightly more houses without storage rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc14894c-c254-4aea-a023-512659273ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT hasStorageRoom, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY hasStorageRoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7ee5242-03f6-4385-8f5b-e25929e86f93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the hasGuestRoom feature, we can see that there are no outliers and the distribution does not seem to be normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defd7922-ef6a-4b3b-82f3-81088785f997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT hasGuestRoom, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY hasGuestRoom\n",
    "ORDER BY hasGuestRoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47c60e57-1ce9-471f-99d4-24f490f4b742",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can look at the distribution of the target variable price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a34db95c-2e54-4734-bccf-fbb7d73dc4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT price, count(*) AS count\n",
    "FROM dataset\n",
    "GROUP BY price\n",
    "ORDER BY price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992e4172-5b39-44e7-bafc-c85464a57a49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Similarly the price varible don't seem do be normally distributed and there are no outliers. \n",
    "\n",
    "We can conclude that most of our distributions are not normally distributed. This is most likely because this is not real data but rather generated data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3a2484-abcf-4df4-b2f9-86f959622959",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can explore the correlation between the features and the target variable using a heatmap. We use seaborn to plot the heatmap as the heatmap graph does not work when using the default Databrick graph feature (it only allows to select 2 features and not the entire dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca0e9e1-a1ce-4823-856a-d0639416a558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_pandas = df.toPandas()#we transform our spark dataframe to a pandas df\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_pandas.corr(), cmap='coolwarm') #we use the seaborn heatmap function \n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec58f50-4898-4f5e-88af-cd8d252beca8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we can see that there are no problems of multicolinearity between features, as none of them are correlated between each other. Then we can notice that the target variable is only correlated to the squareMeters feature and no other features, which might be problematic for our models.  \n",
    "\n",
    "We can explore that correlation better using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3a5550-196a-4c35-8c37-727eb8c674fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT price, squaremeters\n",
    "FROM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9aeae71-c8d1-414b-b246-5516d03a9a33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As indicated  by the correlation of around 0.6 on the heatmap, we can see on the scatter plot that the squreMeters feature is extremely correlated to the target variable. This will cause overfitting problems for our models. However if we remove this feature we get bad  results.\n",
    "\n",
    "Now we can remove the outliers from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f69d8d7-3d52-4253-89b7-bcb9b5462ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_without_outliers = df.subtract(outliers)\n",
    "df_without_outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a627d8d-890c-488b-80c7-fd621e22468e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_without_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e38aa1f-ced4-47ea-8b23-04e67822f716",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Selection\n",
    "\n",
    "As we have a large number of columns we can apply feature selection to reduce the number of features used to train our models.\n",
    "We will use the UnivariateFeatureSelector (based on the f value) to rank the most meaningful features and take only the 10 most important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d22323-0782-475e-8342-f3a0a04a8d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import UnivariateFeatureSelector, VectorAssembler\n",
    "\n",
    "#we take all the features\n",
    "columns = [\"squareMeters\", \"numberOfRooms\", \"hasYard\", \"hasPool\", \"floors\", \"cityCode\", \"cityPartRange\", \"numPrevOwners\", \"made\", \"isNewBuilt\", \"hasStormProtector\", \"basement\", \"attic\", \"garage\", \"hasStorageRoom\", \"hasGuestRoom\"]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")# we use the vector assembler to put all our features in a single vector\n",
    "vect_df = vector_assembler.transform(df_without_outliers)\n",
    "vect_df = vect_df.select(\"features\", col(\"price\").alias(\"label\"))# we add our target varibale price renamed as label\n",
    "\n",
    "selector = UnivariateFeatureSelector(outputCol=\"selectedFeatures\")\n",
    "selector.setFeatureType(\"continuous\").setLabelType(\"continuous\").setSelectionThreshold(10) # our features and target are continous varibles and we select a threshold of 10 to take the 10 most important features\n",
    "model = selector.fit(vect_df)\n",
    "print(\"Selected Features:\", model.selectedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bf4695-a3cb-40f5-b31d-f0ff56a52948",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_column_index = len(df.columns) - 1\n",
    "selected_features = [0, 1, 4, 8, 13, 11, 5, 10, 6, 15, last_column_index]#we define the indexes of the selected columns and the target variable\n",
    "\n",
    "df_reduced_features = df_without_outliers.select(*[col(df_without_outliers.columns[i]) for i in selected_features])# we create a new datframe containg only the selcted features and the target and withut the outliers\n",
    "display(df_reduced_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da59b840-569e-44b3-b5f1-e83a3f5e1188",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we split our data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2558ea2-0d09-4da3-b0f1-4f9414fca0bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df_reduced_features = df_reduced_features.withColumnRenamed(\"price\", \"label\") #we rename the price column to label for our future pipeline\n",
    "train_df, test_df = df_reduced_features.randomSplit([0.7, 0.3], seed=42)  # we split the dataset 70/30 and set seed to 42 for reproducibility\n",
    "\n",
    "print(train_df.count(), test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb3b884-8e0f-4c7d-b46d-ea823abe04a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9867bc7-09ad-4e0f-ac3c-76990ff655de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 3: Create models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44975b8-2da4-4a0b-8325-331277fbf9d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In terms of preprocessing we will only scale our data(using normalization) as that is an important step for regression tasks and allows to get better models. Linear Regression models are particularly sensitive to unscaled data, while decision trees and ensemble methods are less affected.\n",
    "\n",
    "To avoid data leakage the test data is scaled on a scaler fitted on the training data only, so that our model does not see the test data distribution during training.  \n",
    "\n",
    "We define a pipeline containing the vector assembler, the scaler and the model chosen. We are testing out 4 different models : Linear Regression, Decision Tree Regressor, Random Forest Regressor and Gradient Boosted Tree Regressor.  \n",
    "\n",
    "We do a 5 fold cross validation to get more reliable results instead of simple hold out testing and a grid search to find the best hyperparameters for each model.  \n",
    "\n",
    "Then we will evaluate each model on multplie different regression metrics : R squared, Root Mean Squared Error, Mean Squared Error, Mean Absolute Error and Explained Varriance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee0479f-1ca5-4ffd-a64f-98ab321e30cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "feature_cols = ['squareMeters', 'numberOfRooms', 'floors', 'made', 'garage', 'basement', 'cityCode', 'hasStormProtector', 'cityPartRange','hasGuestRoom', 'label'] # we take only the selected most relevant columns\n",
    "\n",
    "# We use a VectorAssembler to combine all the features into a single vector\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# We use a StandardScaler to scale the features to the standard normal distribution (by removing the mean and scaling to unit variance)\n",
    "scaler = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "#We define our Linear Regression Model\n",
    "lr = LinearRegression(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "#We define our parameter grid for hyperparameter tuning for LR taking the most important hyperparameters (explainations of each hyperparameters based on the official documentation)\n",
    "param_grid_lr = (ParamGridBuilder()\n",
    "            .addGrid(lr.loss, [\"squaredError\"]) # already the default parameter: The loss function to be optimized\n",
    "            .addGrid(lr.regParam, [0.01, 0.1, 1.0]) # Regularization parameter (>= 0), penalizes higher coefficients\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) # The ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty\n",
    "            .build())\n",
    "\n",
    "# We define a pipeline that combines all the stages\n",
    "pipeline_lr = Pipeline(stages=[vector_assembler, scaler, lr])\n",
    "\n",
    "#We define RegressionEvaluators for each of the regression metrics we will test our models on\n",
    "evaluator_r2 = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator_rmse = RegressionEvaluator(metricName=\"rmse\")\n",
    "evaluator_mse = RegressionEvaluator(metricName=\"mse\")\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\")\n",
    "evaluator_var = RegressionEvaluator(metricName=\"var\")\n",
    "\n",
    "#we use a 5 fold cross validation to find the optimized model with the best hyperparameters. The best model is chosen based on the RMSE (the model with the lowest RMSE).  We use parallelism to make execution faster and take advantage of spark's distributed nodes\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr, evaluator=evaluator_rmse, estimatorParamMaps=param_grid_lr, numFolds=5, parallelism = 4)\n",
    "\n",
    "# We fit the model on the training data\n",
    "model_lr = crossval_lr.fit(train_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f356e5-a44e-46dd-a98a-353b40de793f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "#We define our Decision Tree Model\n",
    "dt = DecisionTreeRegressor(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "#We define our parameter grid for hyperparameter tuning for DT taking the most important hyperparameters (explainations of each hyperparameters based on the official documentation)\n",
    "param_grid_dt = (ParamGridBuilder()\n",
    "            .addGrid(dt.maxDepth, [3, 5, 8, 10]) # Maximum depth of the tree\n",
    "            .addGrid(dt.minInstancesPerNode, [1, 5, 10]) # Minimum number of instances each child must have after split, if a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid\n",
    "            .addGrid(dt.minInfoGain, [0.0, 0.1, 0.2]) # Minimum information gain for a split to be considered at a tree node\n",
    "            .build())\n",
    "\n",
    "# We define a pipeline that combines all the stages\n",
    "pipeline_dt = Pipeline(stages=[vector_assembler, scaler, dt])\n",
    "\n",
    "#we use a 5 fold cross validation to find the optimized model with the best hyperparameters. The best model is chosen based on the RMSE (the model with the lowest RMSE).  We use parallelism to make execution faster and take advantage of spark's distributed nodes\n",
    "crossval_dt = CrossValidator(estimator=pipeline_dt, evaluator=evaluator_rmse, estimatorParamMaps=param_grid_dt, numFolds=5, parallelism = 4)\n",
    "\n",
    "# We fit the model on the training data\n",
    "model_dt = crossval_dt.fit(train_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "666f4ea0-8973-4af7-a908-51e2f4c21fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "#We define our Random Forest Model\n",
    "rf = RandomForestRegressor(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "#We define our parameter grid for hyperparameter tuning for RF taking the most important hyperparameters (explainations of each hyperparameters based on the official documentation)\n",
    "param_grid_rf = (ParamGridBuilder()\n",
    "            .addGrid(rf.numTrees, [10, 50, 100]) # Number of trees to train\n",
    "            .addGrid(rf.maxDepth, [3, 5, 8, 10]) # maxDepth and minInfoGain are the same as for Decision Tree Regressor\n",
    "            .addGrid(rf.minInfoGain, [0.0, 0.1, 0.2]) \n",
    "            .build())\n",
    "\n",
    "# We define a pipeline that combines all the stages\n",
    "pipeline_rf = Pipeline(stages=[vector_assembler, scaler, rf])\n",
    "\n",
    "#we use a 5 fold cross validation to find the optimized model with the best hyperparameters. The best model is chosen based on the RMSE (the model with the lowest RMSE).  We use parallelism to make execution faster and take advantage of spark's distributed nodes\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf, evaluator=evaluator_rmse, estimatorParamMaps=param_grid_rf, numFolds=5, parallelism = 4)\n",
    "\n",
    "# We fit the model on the training data\n",
    "model_rf = crossval_rf.fit(train_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7a8e03-f28f-4fad-9173-e9726ed09aca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "#We define our Gradient-Boosted Tree Model\n",
    "gbt = GBTRegressor(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "#We define our parameter grid for hyperparameter tuning for DBT taking the most important hyperparameters (explainations of each hyperparameters based on the official documentation)\n",
    "param_grid_gbt = (ParamGridBuilder()\n",
    "        # we remove maxDepth and minInfoGain as the grid search takes too much time\n",
    "        #    .addGrid(gbt.maxDepth, [3, 5, 8, 10]) # maxDepth and minInfoGain are similar hyperparameters as for DT and RF\n",
    "        #    .addGrid(gbt.minInfoGain, [0.0, 0.1, 0.2])\n",
    "            .addGrid(gbt.maxIter, [10, 20, 50]) # Max number of iterations (>= 0)\n",
    "            .addGrid(gbt.stepSize, [0.1, 0.05]) # Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator\n",
    "            .build())\n",
    "\n",
    "# We define a pipeline that combines all the stages\n",
    "pipeline_gbt = Pipeline(stages=[vector_assembler, scaler, gbt])\n",
    "\n",
    "#we use a 5 fold cross validation to find the optimized model with the best hyperparameters. The best model is chosen based on the RMSE (the model with the lowest RMSE).  We use parallelism to make execution faster and take advantage of spark's distributed nodes\n",
    "crossval_gbt = CrossValidator(estimator=pipeline_gbt, evaluator=evaluator_rmse, estimatorParamMaps=param_grid_gbt, numFolds=5, parallelism = 4)\n",
    "\n",
    "# We fit the model on the training data\n",
    "model_gbt = crossval_gbt.fit(train_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790fd2e3-3fbb-4fe6-a3e6-65a7b982a2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 4: Model Evaluation and Explanations of Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3cd5ee3-4d4e-474f-b748-7b3930520689",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will evaluate and compare our models based on the regression metrics (R squared, Adjusted R squared, Root Mean Squared Error, Mean Squared Error, Mean Absolute Error and Explained Varriance).  \n",
    "\n",
    "Then we will have a look at the predictions for each model and plot a comparaison between the predictions and the real labels, the residuals plot and the distribution of the residuals.  \n",
    "\n",
    "For the optimized linear regression model we have the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71887773-57b5-43d0-857b-3119c6b99d05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_pred_df_lr = model_lr.transform(test_df) #we get the predictions on the test set\n",
    " \n",
    "r2 = evaluator_r2.evaluate(cv_pred_df_lr)# we get R2 and calculate the adjusted R2\n",
    "n = test_df.count()# n is the total sample size, the number of rows in the test set\n",
    "p = len(test_df.columns) - 1 #p is the number of independant features, we do len of the columns -1 to remove the target column\n",
    "r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - p) #formula for adjusted r squared\n",
    " \n",
    "# We print the regression metrics for the Linear Regression model\n",
    "print(f\"Coefficient of Determination (R Squared): {r2}\")\n",
    "print(f\"Adjusted R Squared: {r2_adjusted}\")\n",
    "print(f\"Root Mean Squared Error: {evaluator_rmse.evaluate(cv_pred_df_lr)}\")\n",
    "print(f\"Mean Squared Error: {evaluator_mse.evaluate(cv_pred_df_lr)}\")\n",
    "print(f\"Mean Absolute Error: {evaluator_mae.evaluate(cv_pred_df_lr)}\")\n",
    "print(f\"Explained Varriance: {evaluator_var.evaluate(cv_pred_df_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76bbcce8-74f5-496c-88c5-f691c6ce4319",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We seem to have very good results as the R2 is very close to 1, however this also a sign of overfitting\n",
    "\n",
    "We can look at the predictions made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3221e69-2b40-42c6-a87c-12aab80d19f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cv_pred_df_lr.select(\"label\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cac8bee-228a-4117-93e2-5709030266c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that for the first test instances the model is not as accuarte but then the label and prediction values come very close.  \n",
    "\n",
    "Now we will plot the regression graphs to evaluate our results better.  \n",
    "\n",
    "We calculate the residuals and add them to the predictions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d204b9a-35a1-44d9-8c30-94083b4d39f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "cv_pred_df_lr = cv_pred_df_lr.withColumn(\"residuals\", F.col(\"label\") - F.col(\"prediction\"))#calculations of the residuals true labels - predicted value\n",
    "\n",
    "display(cv_pred_df_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d885cb3-d88a-4505-aa29-d0f0bab6dbef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We create an SQL table for the LR predictions to make plots using DataBrick's graphing feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc832775-d17d-4e1c-a453-da368daa2c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_pred_df_lr.createOrReplaceTempView(\"lr_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72884275-c8d0-4370-b46d-ddafa3a9f532",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT label, prediction\n",
    "FROM lr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a54178-b018-42f7-9ff2-f662ef802d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT residuals, prediction\n",
    "FROM lr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae01cac-b58a-47ce-bad4-6ef7f326c832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT residuals, count(*) as count\n",
    "FROM lr_predictions\n",
    "GROUP BY residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f203ba7a-2425-46d6-b632-6f70c50cfe79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see, when using the basic functionalities, we can't properly customize our graphs, so we will use matplotlib and seaborn instead. Matplotlib allows us to ajust the x and y axis to see our visuals better and by default suggests the best view(zoom) to see our data and seaborn offers graph functions to get better looking scatter plots and histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297b4743-5b27-4bd8-a555-b6db968e44c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_lr = cv_pred_df_lr.select([\"label\", \"prediction\"]).toPandas()#we convert our predictions and labels into a pandas dataframe for plotting\n",
    "\n",
    "x_label = \"Actual Value\"\n",
    "y_label = \"Predicted Value\"\n",
    "\n",
    "# We use seaborn to create a scatter plot fitted with a regression line\n",
    "sns.regplot(x=\"label\", y=\"prediction\", data=pandas_df_lr)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.xlim(0, 10000000) # we adjust the x and y axis to see the linearity better\n",
    "plt.ylim(0, 10000000)\n",
    "plt.title(\"Linear Regression: Actual vs. Predicted Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a2b1dde-7ef3-4e66-973b-a0a1f1964c7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Our predictions and the actual values are very correlated which means we are correctly predicting the values. However this is also a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39042e9d-5437-487e-a798-9262e16cdae7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_lr[\"residual\"] = pandas_df_lr[\"label\"] - pandas_df_lr[\"prediction\"]# we calculate the residuals\n",
    "\n",
    "x_label = \"Predicted Value\"\n",
    "y_label = \"Residuals\"\n",
    "\n",
    "sns.regplot(x=\"prediction\", y=\"residual\", data=pandas_df_lr) #we use the regplot to get the scatter plot, the regression fitted line and confidance interval\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.ylim(-500, 500)#we adjust the y axis to zoom in more\n",
    "plt.title(\"Linear Regression : Residuals Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6f8497-b9fe-4e43-978b-d429cb0996bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The residuals plot shows that the residuals are focussed around 0 and scattered randomly (with some outliers further away), indicating that the model's errors are unbiased and independent of the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c380a6-5092-4188-b341-2effbf12d3d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(pandas_df_lr[\"residual\"])# seaborn provides a more smoothed version of the histogram\n",
    "\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(-25, 25)\n",
    "plt.title(\"Linear Regression : Distribution of the Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256ecaa4-63e1-4c11-b9fa-60ca6b302b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we can see that the residuals distribution is approximately normal, which means that the errors are symmetrical and unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dbbe3f-8226-4774-952c-c6f1a98e9c02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can have a look at the Decsion tree model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad805260-1725-4818-ab46-4f95b8069cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_pred_df_dt = model_dt.transform(test_df) #we get the predictions on the test set\n",
    "\n",
    "r2 = evaluator_r2.evaluate(cv_pred_df_dt)# we get R2 and calculate the adjusted R2\n",
    "n = test_df.count()# n is the total sample size, the number of rows in the test set\n",
    "p = len(test_df.columns) - 1 #p is the number of independant features, we do len of the columns -1 to remove the target column\n",
    "r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - p)#formula for adjusted r squared\n",
    " \n",
    "# We print the regression metrics for the Decision Tree model\n",
    "print(f\"Coefficient of Determination (R Squared): {r2}\")\n",
    "print(f\"Adjusted R Squared): {r2_adjusted}\")\n",
    "print(f\"Root Mean Squared Error: {evaluator_rmse.evaluate(cv_pred_df_dt)}\")\n",
    "print(f\"Mean Squared Error: {evaluator_mse.evaluate(cv_pred_df_dt)}\")\n",
    "print(f\"Mean Absolute Error: {evaluator_mae.evaluate(cv_pred_df_dt)}\")\n",
    "print(f\"Explained Varriance: {evaluator_var.evaluate(cv_pred_df_dt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91c7256e-fb12-4da6-871a-7e77550574ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Similarly to the previous model, we get a very high R squared which means the model is likely to be overfitting.  \n",
    "\n",
    "We can also see that the predictions seem quite close to the true labels, though as for the linear regression model the first few hundred instances are inacurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c938ec74-206b-4dc5-948e-6a6e090877bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cv_pred_df_dt.select(\"label\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2534520-f9b9-4e7a-95a9-740c9cb9bb76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can plot our regression related graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94a421a-be69-4de7-a6a3-d5fd03819731",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_dt = cv_pred_df_dt.select([\"label\", \"prediction\"]).toPandas() #we convert our predictions and labels into a pandas dataframe for plotting\n",
    "\n",
    "x_label = \"Actual Value\"\n",
    "y_label = \"Predicted Value\"\n",
    "\n",
    "# We use seaborn to create a scatter plot fitted with a regression line\n",
    "sns.regplot(x=\"label\", y=\"prediction\", data=pandas_df_dt)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(\"Decision Tree : Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c91fe30-75ba-4eb5-8b53-dda9f9c47d86",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The true values and the predicted values seem correlated, but there is more variation(or error) than in the previous model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708cd558-3451-4271-b1bf-ff6fd373e896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pandas_df_dt[\"residual\"] = pandas_df_dt[\"label\"] - pandas_df_dt[\"prediction\"] # we calculate the residuals\n",
    "\n",
    "x_label = \"Predicted Value\"\n",
    "y_label = \"Residuals\"\n",
    "\n",
    "sns.regplot(x=\"prediction\", y=\"residual\", data=pandas_df_dt)#we use the regplot to get the scatter plot, the regression fitted line and confidance interval\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(\"Decision Tree : Residuals Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e14e77-d6e7-43d2-8fd7-e6ef44e67af8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The residuals are scattered randomly around 0, but there are bigger error margins/varriation than for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e58b79-3068-429c-9f9c-42f9eaf8ff3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(pandas_df_dt[\"residual\"])# seaborn provides a more smoothed version of the histogram\n",
    "\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Decision Tree: Distribution of the Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee4826d-f3e9-45ed-bfac-25a753c57499",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The residuals seem approximately normally distributed so the errors made by the model are at random and unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fba503-e448-4efc-8720-9e13a522f6ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can move onto the Random Forsest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae5e6fa-7e76-4e4c-b1d9-5f1b8963ebcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_pred_df_rf = model_rf.transform(test_df)\n",
    "\n",
    "r2 = evaluator_r2.evaluate(cv_pred_df_rf)\n",
    "n = test_df.count()\n",
    "p = len(test_df.columns) - 1\n",
    "r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - p)\n",
    " \n",
    "print(f\"Coefficient of Determination (R Squared): {r2}\")\n",
    "print(f\"Adjusted R Squared: {r2_adjusted}\")\n",
    "print(f\"Root Mean Squared Error: {evaluator_rmse.evaluate(cv_pred_df_rf)}\")\n",
    "print(f\"Mean Squared Error: {evaluator_mse.evaluate(cv_pred_df_rf)}\")\n",
    "print(f\"Mean Absolute Error: {evaluator_mae.evaluate(cv_pred_df_rf)}\")\n",
    "print(f\"Explained Varriance: {evaluator_var.evaluate(cv_pred_df_rf)}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08fb8cbd-6ecb-4819-8701-03d529979201",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We get similar results, this model also seems to be overfitting given the very high R squared.  \n",
    "\n",
    "When looking at the predictions they seem mostly close to the true values(except for the first few hundred of test examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85db736d-9a60-4977-bbe7-0648f83b3ad3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cv_pred_df_rf.select(\"label\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d9cc26-5414-4a9e-b6de-f2666e2a43d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_rf = cv_pred_df_rf.select([\"label\", \"prediction\"]).toPandas()\n",
    "\n",
    "x_label = \"Actual Value\"\n",
    "y_label = \"Predicted Value\"\n",
    "\n",
    "sns.regplot(x=\"label\", y=\"prediction\", data=pandas_df_rf)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(\"Random Forest : Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6822d95-cb57-442d-a7fc-3cbf3e77d8df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The true values and the predicted values are correlated, however there are more outliers than for the previous 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6233904-9330-4ca8-a08d-706735896209",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_rf[\"residual\"] = pandas_df_rf[\"label\"] - pandas_df_rf[\"prediction\"]\n",
    "\n",
    "x_label = \"Predicted Value\"\n",
    "y_label = \"Residuals\"\n",
    "\n",
    "sns.regplot(x=\"prediction\", y=\"residual\", data=pandas_df_rf)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.ylim(-1000000, 1000000)\n",
    "plt.title(\"Random Forest : Residuals Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a4f3752-9592-4b22-8dd7-6ae9138d1eae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Looking at the residuals plot, we can see that the residuals are scattered around 0 and we can notice a few points which are a lot further than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1a92f5-0f62-40b0-bd6a-8473b0600e68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(pandas_df_rf[\"residual\"])\n",
    "\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(-1000000, 1000000)\n",
    "plt.title(\"Random Forest: Distribution of the Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f289b2e5-d4cf-47a4-b479-8fd4bcbaaa82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The residuals seem approximately normally distributed, so the errors made by the model don't show any underlying problems.    \n",
    "\n",
    "\n",
    "Simialrly we have for the Gradient Boosted Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16ba1cc-6d57-4b88-9c52-619191a2ee20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_pred_df_gbt = model_gbt.transform(test_df)\n",
    "\n",
    "r2 = evaluator_r2.evaluate(cv_pred_df_gbt)\n",
    "n = test_df.count()\n",
    "p = len(test_df.columns) - 1\n",
    "r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - p)\n",
    " \n",
    "print(f\"Coefficient of Determination (R Squared): {r2}\")\n",
    "print(f\"Adjusted R Squared: {r2_adjusted}\")\n",
    "print(f\"Root Mean Squared Error: {evaluator_rmse.evaluate(cv_pred_df_gbt)}\")\n",
    "print(f\"Mean Squared Error: {evaluator_mse.evaluate(cv_pred_df_gbt)}\")\n",
    "print(f\"Mean Absolute Error: {evaluator_mae.evaluate(cv_pred_df_gbt)}\")\n",
    "print(f\"Explained Varriance: {evaluator_var.evaluate(cv_pred_df_gbt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a0bce3-2292-41e2-8be3-43859506e8f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We get similar results to the previous models, with a very high R squared close to 1, meaning that the model is likely to be overfitting.  \n",
    "\n",
    "As for the other models the predictions seem close except for the first few hundred rows of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe686def-b611-4aea-bbfc-732bbb12797e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cv_pred_df_gbt.select(\"label\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e252daec-d692-4e5b-869f-f2af21609a22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_gbt = cv_pred_df_gbt.select([\"label\", \"prediction\"]).toPandas()\n",
    "\n",
    "x_label = \"Actual Value\"\n",
    "y_label = \"Predicted Value\"\n",
    "\n",
    "sns.regplot(x=\"label\", y=\"prediction\", data=pandas_df_gbt)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(\"Gradient Boosted Tree : Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "549fd498-2be4-4db4-a2d1-d22b7bedda15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The true values and the predicted values are correlated, we get very similar results to our Decison Tree and Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048356f3-1851-4a29-bd92-2bb6a70a5b60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_gbt[\"residual\"] = pandas_df_gbt[\"label\"] - pandas_df_gbt[\"prediction\"]\n",
    "\n",
    "x_label = \"Predicted Value\"\n",
    "y_label = \"Residuals\"\n",
    "\n",
    "sns.regplot(x=\"prediction\", y=\"residual\", data=pandas_df_gbt)\n",
    "\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(y_label)\n",
    "plt.title(\"Gradient Boosted Tree : Residuals Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3d0c2d-11eb-458a-a752-d8c5e624e192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The residual plot shows us that the residuals are scattered around 0, with a lot of variation similar to the Decision Tree model(but slightly better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4c275b-f572-4ac3-a2d5-3710475874b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(pandas_df_gbt[\"residual\"])\n",
    "\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Gradient Boosted Tree: Distribution of the Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f56545d-1686-4b93-bfa7-6b4bdbca33cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Similarly to our previous models, the residuals are approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55229a70-f923-44a9-9d4f-ddddcd94a84f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When comparing the metrics of our models, we can notice a few things. The Decision Tree, Random Forest, Gradient Boosted Tree\n",
    "have very similar metrics to each other as they come from the same family of models. All 4 models have very similar R2 and ajusted R2 values as well as explained variance. In term of RMSE, MSE and MAE the Linear Regression model performs worse than the others.   \n",
    "\n",
    "The results found by ensemble methods like Random Forest are the most reliable as it builds multiple trees and is more likely to capture the variation of the target variable. So we can consider it as our best model.\n",
    "\n",
    "As mentioned many times, our models seem to be overfitting (the R2 is very close to 1). We have previously noticed that the target variable is very correlated to the squareMeters feature, so we will try to remove that feature and see if we get more reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d053f69d-7c57-4e1a-9b4f-7a7aaed01478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols2 = [\"numberOfRooms\", \"garage\", \"floors\", \"basement\", \"made\", \"cityCode\", \"hasStormProtector\", \"cityPartRange\", \"hasGuestRoom\"] # we remove squareMeters\n",
    "\n",
    "#we follow the same steps as previously\n",
    "vector_assembler2 = VectorAssembler(inputCols=feature_cols2, outputCol=\"features\")\n",
    "\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "rf2 = RandomForestRegressor(labelCol=\"label\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "pipeline_rf2 = Pipeline(stages=[vector_assembler2, scaler2, rf2])\n",
    "\n",
    "model_without_squareMeters = pipeline_rf2.fit(train_df)\n",
    " \n",
    "predictions_without_squareMeters = model_without_squareMeters.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4457efae-0db0-4442-ad54-1a54abceccdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the predictions are very wrong and completely different than the true value even for rows further down in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f9744b4-30eb-4651-9abf-a861b3554055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions_without_squareMeters.select([\"label\", \"prediction\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9d36dc-d37f-4adb-9b5c-0c4f315f1dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When looking at the evalution metrics, we can see the results are terrible and the model is underfitting. The R2 is very low, only 0.1 which means our model is not able to capture the variance in the dataset and our features are not able to explain the variation of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35634d66-2cae-4a9b-b2c5-f9a9c3c27200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "r2 = evaluator_r2.evaluate(predictions_without_squareMeters)\n",
    "n = test_df.count()\n",
    "p = len(test_df.columns) - 1 \n",
    "r2_adjusted = 1 - (1 - r2) * (n - 1) / (n - p)\n",
    " \n",
    "print(f\"Coefficient of Determination (R Squared): {r2}\")\n",
    "print(f\"Adjusted R Squared: {r2_adjusted}\")\n",
    "print(f\"Root Mean Squared Error: {evaluator_rmse.evaluate(predictions_without_squareMeters)}\")\n",
    "print(f\"Mean Squared Error: {evaluator_mse.evaluate(predictions_without_squareMeters)}\")\n",
    "print(f\"Mean Absolute Error: {evaluator_mae.evaluate(predictions_without_squareMeters)}\")\n",
    "print(f\"Explained Varriance: {evaluator_var.evaluate(predictions_without_squareMeters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144687d6-5586-4277-9a71-21be0a428e00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When trying with the other models, we also get a similar result, so we can conclude that the squareMeters feature is necessary and we cannot exclude it. The squareMeters feature is very correlated to our target variable, most probably beacause of the construction of the dataset as it it synthetic data and not real life data. So it is still better to take our overfitting models, even though they won't be able to properly generalize on unseen real life data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41085d12-dde8-4800-b6a8-28ea2ab34e85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part 5: Discussion of work completed, competition outcomes, and highlighting features of Spark used\n",
    "This assignment used PySpark to perform a classic machine learning task, predicting house prices. We trained multiple different types of regression models and found the best models for each through a grid search to optimize the hyperparameters. Our best model was the Random Forest Regressor.\n",
    "\n",
    "For the grid search we could have tunned more different hyperparameters or added more posiible values in our parameter grids, however that would have taken a long time (and not necessarily realistic as we are running it on databricks given that for executions longer than 1 hour the cluster is terminated).\n",
    "\n",
    "To prevent overfitting we could have tried to performing early stopping, using different regularization techniques for linear regression, or performing pruning on our tree regressors. However, overfitting is most likely due to the nature of the data as we have see that the squareMeters column and the target variable are highly correlated. The other features are not meaningful enough, so we can't build a godd model with the remaining features.\n",
    "\n",
    "I tried uploading my file for the Kaggle competition, but it did not work as there is no option for a Spark environment. It allowed me to upload my notebook but it look a very long time to process it and the page was unresponive multiple times. In the end it did not even work as seen on the screenshot. \n",
    "\n",
    "A few features of Spark were used for this assignment: the distributed processing, the machine learning librarires (MLlib) and the graphing tools provided for SQL querries. The parallelism feature built into Spark allows to run large datasets on multiple nodes to improve efficiency and shorten execution time."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4279512446719114,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Big Data Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
